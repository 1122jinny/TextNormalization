{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "# Reference : https://github.com/jbhoosreddy/spellcorrect\n",
    "# 4 Confusion matrix (addition, substituion, reversal, deletion) files\n",
    "# English corpus file\n",
    "# English dictionary file\n",
    "# Errors of spell file\n",
    "# Above things are referred to that github repository as written.\n",
    "\n",
    "# This program is to correct non-word spelling error in sentences using ngram MAP language models,\n",
    "# noisy channel model, error confusion matrix and Damerau-Levenshtein edit distance.\n",
    "# Usage --\n",
    "# Input : she is a briliant acress\n",
    "# Response : She is a brilliant actress\n",
    "\n",
    "from __future__ import division\n",
    "from ngram import nGram \n",
    "import ast                       # To read confusion matrix files\n",
    "import hgtk\n",
    "import math as calc              # To calculate mathematical forms\n",
    "import locale\n",
    "import functools\n",
    "import re\n",
    "import pandas as pd\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "from pykospacing import spacing\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "# Class name : SpellCorrect\n",
    "import preprocessing\n",
    "from konlpy.corpus import kolaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Corpus from data file\n",
      "Processing Corpus\n",
      "Creating Unigram Model\n",
      "Calculating Count for Unigram Model\n",
      "Creating Trigram Model\n",
      "Calculating Count for Trigram Model\n",
      "Creating Bigram Model\n",
      "Calculating Count for Bigram Model\n",
      "['안녕하세요']\n",
      "현재 검색 단어 : 안녕하세요\n",
      "Response:  안녕하세요\n",
      "['수험생', '여ㄹ러분']\n",
      "현재 검색 단어 : 수험생\n",
      "현재 검색 단어 : 여ㄹ러분\n",
      "Response:  수험생 여러분\n",
      "['어느새', '십험이', '왔네요']\n",
      "현재 검색 단어 : 어느새\n",
      "현재 검색 단어 : 십험\n",
      "현재 검색 단어 : 왔네요\n",
      "Response:  어느새 시험이 왔네요\n",
      "['그동안', '고생', '많으ㅅ셨습니다']\n",
      "현재 검색 단어 : 그동안\n",
      "현재 검색 단어 : 고생\n",
      "현재 검색 단어 : 많으ㅅ셨습니다\n",
      "Response:  그동안 고생 많으셨습니다\n",
      "그동안 고생 많으셨습니다\n"
     ]
    }
   ],
   "source": [
    "class NoisyChannelModel():\n",
    "\n",
    "    # Method : __init__(self)\n",
    "    # A constructor to declare ngram language model, load words, confusion matrix and dictionary.\n",
    "    def __init__(self):\n",
    "        locale.setlocale(locale.LC_ALL, '') #한국어 기준으로 set\n",
    "\n",
    "        # Construct nGram Language model for 1-gram, bi-gram \n",
    "        self.ng = nGram(uni = True,bi = True)\n",
    "  #      self.ng_word =  self.open_pre_ngram()\n",
    "\n",
    "        # During constructing nGram language model,\n",
    "        # it turn textwords corpus into \"set\" data structure\n",
    "        # For instance, set(\"Hello\") --> {'e', 'H', 'l', 'o'}\n",
    "        # As same, it makes the words corpus as set data structure and sort those things.\n",
    "        self.words = sorted(set(self.ng.words),key=functools.cmp_to_key(locale.strcoll))\n",
    "        #ng_word\n",
    "        # Read 4 confusion matrix external text file and assign it to its ~~~matrix variables.\n",
    "        self.loadConfusionMatrix()\n",
    "\n",
    "        # Read English dictionaly external text file and split it into word by enter character '\\n'\n",
    "        self.dict = self.loadDict()\n",
    "\n",
    "        # End method\n",
    "        return\n",
    "    \n",
    "    \"\"\"def open_pre_ngram(self):\n",
    "        f =open('pre_ngram_words.txt','r')\n",
    "        word = f.read().split('\\n')\n",
    "        return word\"\"\"\n",
    "    \n",
    "    # Method : loadConfusionMatrix(self)\n",
    "    # A method to load confusion matrix from external confusion matrix data file\n",
    "    def loadConfusionMatrix(self):\n",
    "        # Read addition confusion matrix from external text file\n",
    "        #df = pd.read_csv('subtestmatrix.txt',encoding='cp949', names=['x','y','count'])\n",
    "        self.addmatrix = pd.read_csv('addtestmatrix.txt',encoding='cp949', names=['x','y','count'])\n",
    "\n",
    "        # Read substitution confusion matrix from external text file\n",
    "        self.submatrix = pd.read_csv('subtestmatrix.txt',encoding='cp949', names=['x','y','count'])\n",
    "\n",
    "        # Read reverse confusion matrix from external text file\n",
    "        self.revmatrix = pd.read_csv('revtestmatrix.txt',encoding='cp949', names=['x','y','count'])\n",
    "\n",
    "        # Read deletion confusion matrix from external text file\n",
    "        self.delmatrix = pd.read_csv('deltestmatrix.txt',encoding='cp949', names=['x','y','count'])\n",
    "\n",
    "    # Method : loadDict(self)\n",
    "    # A method to load dictionary file in directory into memory\n",
    "    def loadDict(self):\n",
    "        #print(\"Loading dictionary from data file\")\n",
    "        f = open('dictionary.txt','r') # ,encoding='cp949'원래 dictionary.txt\n",
    "        #f = open('semi_dictionary.txt','r',encoding='utf-8') # Test용 'ㅇ' 부분만 자른 semi_ictionary.txt\n",
    "        return f.read().split(\"\\n\")\n",
    "    \n",
    "        \n",
    "    # Method : editDistanceOfDamerauLevenshtein(self, s1, s2)\n",
    "    # A method to calculate Damerau-Levenshtein which expands minimum edit distance method for given two strings.\n",
    "    def editDistanceOfDamerauLevenshtein(self, source, target):  #내가 입력한 단어, 후보 검사할 단어\n",
    "        # Concatenate '#' to two strings\n",
    "        # Get each length of two strings\n",
    "        source = '#' + source\n",
    "        target = '#' + target\n",
    "        #print('source : ', source)\n",
    "        #print('target : ', target)\n",
    "        source = re.sub('ᴥ','',source)\n",
    "        target = re.sub('ᴥ','',target)\n",
    "        lengthOfSource = len(source) #내가 입력한 단어 자소분리 후 길이 저장\n",
    "        lengthOfTarget = len(target) #후보 검사할 단어 자소분리 후 길이 저장\n",
    "        #print('lengthOfSource : ', lengthOfSource)\n",
    "        #print('lengthOfTarget : ', lengthOfTarget)\n",
    "        \n",
    "        # Calculate Damerau-Levenshtein edit distance matrix in distanceMatrix\n",
    "        distanceMatrix = [[0]*lengthOfTarget for i in range(lengthOfSource)]\n",
    "        for i in range(lengthOfSource):\n",
    "             for j in range(lengthOfTarget):\n",
    "                    distanceMatrix[i][0] = i\n",
    "                    distanceMatrix[0][j] = j\n",
    "                 \n",
    "        for i in range(lengthOfSource):\n",
    "            for j in range(lengthOfTarget):\n",
    "                # distance variable to calculate distance between source and target string\n",
    "                distance = [0] * 4\n",
    "                if i == 0 or j == 0:\n",
    "                    continue\n",
    "                # Assign the values in distance matrix\n",
    "                distance[0] = distanceMatrix[i-1][j] + 1\n",
    "                distance[1] = distanceMatrix[i][j-1] + 1\n",
    "                # Select the minimum value for making chart of minimum edit distance according to Damerau-Levenshtein edit distance algorithm\n",
    "                if source[i] != target[j]:\n",
    "                    distance[2] = distanceMatrix[i-1][j-1] + 2\n",
    "                else:\n",
    "                    distance[2] = distanceMatrix[i-1][j-1]\n",
    "                if source[i] == target[j-1] and source[i-1] == target[j]:\n",
    "                    distance[3] = distanceMatrix[i-1][j-1] - 1\n",
    "                if distance[3] != 0:\n",
    "                    distanceMatrix[i][j] = min(distance[0:4])\n",
    "                else:\n",
    "                    distanceMatrix[i][j] = min(distance[0:3])\n",
    "                    \n",
    "        # Return distance value matrix between source and target strings\n",
    "        return distanceMatrix[lengthOfSource-1][lengthOfTarget-1]\n",
    "    \n",
    "    def splitSentence(self, sentence):\n",
    "        komoran = Komoran()\n",
    "        sentence = komoran.morphs(sentence)\n",
    "        return sentence\n",
    "        \n",
    "    def splitWord(self, word):\n",
    "        word = re.sub('ㅙ','ㅗㅐ',word)\n",
    "        word = re.sub('ㅘ','ㅗㅏ',word)\n",
    "        word = re.sub('ㅚ','ㅗㅣ',word)\n",
    "        word = re.sub('ㅞ','ㅜㅔ',word)\n",
    "        word = re.sub('ㅟ','ㅜㅣ',word)\n",
    "        word = re.sub('ㅞ','ㅜㅔ',word)\n",
    "        word = re.sub('ㅝ','ㅜㅓ',word)\n",
    "        word = re.sub('ㅢ','ㅡㅣ',word)\n",
    "        word = re.sub('ㅖ','ㅕㅣ',word)\n",
    "        word = re.sub('ㅒ','ㅑㅣ',word)\n",
    "        word = re.sub('ㄲ','ㄱㄱ',word)\n",
    "        word = re.sub('ㄸ','ㄷㄷ',word)\n",
    "        word = re.sub('ㅃ','ㅂㅂ',word)\n",
    "        word = re.sub('ㅆ','ㅅㅅ',word)\n",
    "        word = re.sub('ㅉ','ㅈㅈ',word)\n",
    "        word = re.sub('ㄳ','ㄱㅅ',word)\n",
    "        word = re.sub('ㄵ','ㄴㅈ',word)\n",
    "        word = re.sub('ㄶ','ㄴㅎ',word)\n",
    "        word = re.sub('ㄺ','ㄹㄱ',word)\n",
    "        word = re.sub('ㄻ','ㄹㅁ',word)\n",
    "        word = re.sub('ㄼ','ㄹㅂ',word)\n",
    "        word = re.sub('ㄽ','ㄹㅅ',word)\n",
    "        word = re.sub('ㄾ','ㄹㅌ',word)\n",
    "        word = re.sub('ㄿ','ㄹㅍ',word)\n",
    "        word = re.sub('ㅀ','ㄹㅎ',word)\n",
    "        word = re.sub('ㅄ','ㅂㅅ',word)\n",
    "        return word\n",
    " \n",
    "    def uniteWord(self, word):\n",
    "        word = re.sub('ㅗㅐ','ㅙ',word)\n",
    "        word = re.sub('ㅗㅏ','ㅘ',word)\n",
    "        word = re.sub('ㅗㅣ','ㅚ',word)\n",
    "        word = re.sub('ㅜㅔ','ㅞ',word)\n",
    "        word = re.sub('ㅜㅣ','ㅟ',word)\n",
    "        word = re.sub('ㅜㅔ','ㅞ',word)\n",
    "        word = re.sub('ㅜㅓ','ㅝ',word)\n",
    "        word = re.sub('ㅡㅣ','ㅢ',word)\n",
    "        word = re.sub('ㅕㅣ','ㅖ',word)\n",
    "        word = re.sub('ㅑㅣ','ㅒ',word)\n",
    "        word = re.sub('ㄱㄱ','ㄲ',word)\n",
    "        word = re.sub('ㄷㄷ','ㄸ',word)\n",
    "        word = re.sub('ㅂㅂ','ㅃ',word)\n",
    "        word = re.sub('ㅅㅅ','ㅆ',word)\n",
    "        word = re.sub('ㅈㅈ','ㅉ',word)\n",
    "        word = re.sub('ㄱㅅ','ㄳ',word)\n",
    "        word = re.sub('ㄴㅈ','ㄵ',word)\n",
    "        word = re.sub('ㄴㅎ','ㄶ',word)\n",
    "        word = re.sub('ㄹㄱ','ㄺ',word)\n",
    "        word = re.sub('ㄹㅁ','ㄻ',word)\n",
    "        word = re.sub('ㄹㅂ','ㄼ',word)\n",
    "        word = re.sub('ㄹㅅ','ㄽ',word)\n",
    "        word = re.sub('ㄹㅌ','ㄾ',word)\n",
    "        word = re.sub('ㄹㅍ','ㄿ',word)\n",
    "        word = re.sub('ㄹㅎ','ㅀ',word)\n",
    "        word = re.sub('ㅂㅅ','ㅄ',word)\n",
    "        return word\n",
    "    \n",
    "    def extractCan(self, word):\n",
    "        # addConfusionMatrix # x가 xy로 쓰여진 경우 / x가 candidate letter, xy가 input letter\n",
    "        candidates = [word]\n",
    "        candidates.append(\"ADD : \")\n",
    "        for i in range(len(word)-1): # 입력 자소 차례로 받아오기\n",
    "            if i == 0:\n",
    "                candidate = word[i+1:]\n",
    "                candidates.append(candidate)\n",
    "            elif i < len(word) - 1:\n",
    "                candidate = word[:i]+word[i+1:] # y부분 제거\n",
    "                candidates.append(candidate)\n",
    "                if candidate[i-1] == 'ᴥ' and candidate[i] == 'ᴥ': # 모음이 추가된 경우\n",
    "                    candidate = word[:i-1]+word[i+2:] \n",
    "                    candidates.append(candidate)\n",
    "                \n",
    "        # subConfusionMatrix # x가 y로 쓰여진 경우 / y : input , x : candidate letter\n",
    "        candidates.append(\"SUB : \")\n",
    "        for i in range(len(word)): # 입력 자소 차례로 받아오기\n",
    "            if word[i] == 'ᴥ':\n",
    "                continue\n",
    "            candidates_letter = self.submatrix[(self.submatrix['y']==word[i])]\n",
    "            candidates_letter = candidates_letter.sort_values(['count'], ascending=[False])\n",
    "            candidates_letter = candidates_letter[0:15]\n",
    "            candidates_letter = candidates_letter['x'].values\n",
    "            # range 수정\n",
    "            for j in range(10):\n",
    "                if i == 0:\n",
    "                    candidate = candidates_letter[j]+word[i+1:]\n",
    "                    candidates.append(candidate)\n",
    "                elif i < len(word)-1:\n",
    "                    candidate = word[:i]+candidates_letter[j]+word[i+1:]\n",
    "                    candidates.append(candidate)\n",
    "            \n",
    "        # revConfusionMatrix # xy가 yx로 쓰여진 경우 / yx는 input xy는 candidate letter\n",
    "        candidates.append(\"TRANS : \") # input에서 word[i] = y, word[i+1] = x\n",
    "        for i in range(len(word)-2): #팩토리얼처럼 모든 trans 경우 구함\n",
    "            if i == 0:\n",
    "                candidate = word[i+1]+word[i]+word[i+2:] # ex: 괏님(관심)처럼 형태가 정확한 경우\n",
    "                candidates.append(candidate)\n",
    "                if i < len(word)-3 and word[i+1] == 'ᴥ':\n",
    "                    candidate = word[i+2]+word[i+1]+word[i]+word[i+3:] # ex: 괏님(관심)처럼 형태가 정확한 경우\n",
    "                    candidates.append(candidate)\n",
    "                if i < len(word)-3 and word[i+1] == 'ᴥ' and word[i+3] == 'ᴥ': #자,모가 바뀌어 ᴥ 모양이 있는 경우\n",
    "                    if i < len(word)-5 and word[i+5] == 'ᴥ': #받침있는 \n",
    "                        candidate = word[i+2]+word[i]+word[i+4:] #자+모(ex:ㅓ,ㅅ,ㅇ,공)\n",
    "                        candidates.append(candidate)\n",
    "                        candidate = word[i]+word[i+4]+word[i+2]+word[i+5:] #모+자(ex:ㅅ,ㅇ,ㅓ,공)\n",
    "                        candidates.append(candidate)\n",
    "                    else: #받침없는 자+모(ex:ㅏ,ㅁ,술), 자+자(받침, ex: 싷,ㄹ,다)\n",
    "                        candidate = word[i+2]+word[i]+word[i+3:]\n",
    "                        candidates.append(candidate)\n",
    "                elif i < len(word)-4 and word[i+1] == 'ᴥ' and word[i+4] == 'ᴥ': #모+자(ex: ㅅ,어,공)\n",
    "                    candidate = word[i]+word[i+3]+word[i+2]+word[i+4:]\n",
    "                    candidates.append(candidate)\n",
    "                        \n",
    "            else:\n",
    "                candidate = word[:i]+word[i+1]+word[i]+word[i+2:] # ex: 괏님(관심)처럼 형태가 정확한 경우\n",
    "                candidates.append(candidate)\n",
    "                if i < len(word)-3 and word[i+1] == 'ᴥ':\n",
    "                    candidate = word[:i]+word[i+2]+word[i+1]+word[i]+word[i+3:] # ex: 괏님(관심)처럼 형태가 정확한 경우\n",
    "                    candidates.append(candidate)\n",
    "                if i < len(word)-3 and word[i+1] == 'ᴥ' and word[i+3] == 'ᴥ': #자,모가 바뀌어 ᴥ 모양이 있는 경우\n",
    "                    if i < len(word)-5 and word[i+5] == 'ᴥ': #받침있는 \n",
    "                        candidate = word[:i]+word[i+2]+word[i]+word[i+4:] #자+모(ex:ㅓ,ㅅ,ㅇ,공)\n",
    "                        candidates.append(candidate)\n",
    "                        candidate = word[:i+1]+word[i+4]+word[i+2]+word[i+5:] #모+자(ex:ㅅ,ㅇ,ㅓ,공)\n",
    "                        candidates.append(candidate)\n",
    "                    else: #받침없는 자+모(ex:ㅏ,ㅁ,술), 자+자(받침, ex: 싷,ㄹ,다)\n",
    "                        candidate = word[:i]+word[i+2]+word[i]+word[i+3:]\n",
    "                        candidates.append(candidate)\n",
    "                elif i < len(word)-4 and word[i+1] == 'ᴥ' and word[i+4] == 'ᴥ': #모+자(ex: ㅅ,어,공)\n",
    "                    candidate = word[:i+1]+word[i+3]+word[i+2]+word[i+4:]\n",
    "                    candidates.append(candidate)\n",
    "                    \n",
    "        # delConfusionMatrix # xy가 x로 쓰여진 경우 / xy가 candidate letter, x가 input letter\n",
    "        candidates.append(\"DEL : \")\n",
    "        for i in range(len(word)-1): # 입력 자소 차례로 받아오기\n",
    "            if word[i] == 'ᴥ':\n",
    "                continue\n",
    "            candidates_letter = self.delmatrix[(self.delmatrix['x']==word[i])]\n",
    "            candidates_letter = candidates_letter.sort_values(['count'], ascending=[False])\n",
    "            candidates_letter = candidates_letter[0:15]\n",
    "            candidates_letter = candidates_letter['y'].values\n",
    "            for j in range(10):\n",
    "                if i == 0:\n",
    "                    if i < len(word)-3 and word[i+1] == 'ᴥ' and word[i+3] == 'ᴥ': # 받침있는 단어의 \n",
    "                        candidate = candidates_letter[j]+word[i]+word[i+2:] #초성이 없는 경우(ex: ㅏ,ㄴ,녕), candidates_letter[j]는 중성과 연관\n",
    "                        candidates.append(candidate)\n",
    "                        candidate = word[i]+candidates_letter[j]+word[i+2:] #중성이 없는 경우(ex: ㅇ,ㄴ,녕), candidates_letter[j]는 초성과 연관\n",
    "                        candidates.append(candidate)\n",
    "                    else: \n",
    "                        candidate = word[i:i+2]+candidates_letter[j]+word[i+2:] # 받침있는 단어의 종성이 없는 경우(ex: 아,녕)\n",
    "                        candidates.append(candidate)\n",
    "                        candidate = candidates_letter[j]+word[i:] # 받침없는 단어의 초성이 없는 경우\n",
    "                        candidates.append(candidate)\n",
    "                        candidate = word[i]+candidates_letter[j]+word[i+1:]  # 받침없는 단어의 중성이 없는 경우\n",
    "                        candidates.append(candidate)\n",
    "                else:\n",
    "                    if i > 1 and i < len(word)-3 and word[i+1] == 'ᴥ' and word[i+3] == 'ᴥ': # 받침있는 단어(초성,중성)\n",
    "                        # 앞글자가 받침이 있을 때 현재 단어의 초성이 없는 경우(ex: 콘,센,트->코,넨,트), candidates_letter[j]는 중성과 연관\n",
    "                        candidate = word[:i-1]+word[i]+word[i-1]+candidates_letter[j]+word[i+1]\n",
    "                        candidates.append(candidate)\n",
    "                        # 앞글자가 받침이 없을 때 현재 단어의 초성이 없는 경우(ex: 커,튼->커,ㅡ,ㄴ), candidates_letter[j]는 중성과 연관\n",
    "                        candidate = word[:i]+candidates_letter[j]+word[i]+word[i+2:] \n",
    "                        candidates.append(candidate)\n",
    "                        \n",
    "                        # 앞글자가 받침이 없을 때 현재 단어의 중성이 없는 경우(ex: 전,자,석->전,잣,ㄱ), candidates_letter[j]는 초성과 연관\n",
    "                        candidate = word[:i]+word[i+1]+word[i]+candidates_letter[j]+word[i+2:]\n",
    "                        candidates.append(candidate)\n",
    "                        # 앞글자가 받침이 있을 때 현재 단어의 중성이 없는 경우(ex: 콘,센,트->콘,ㅔ,ㄴ,트), candidates_letter[j]는 초성과 연관\n",
    "                        candidate = word[:i]+word[i]+candidates_letter[j]+word[i+2:] \n",
    "                        candidates.append(candidate)\n",
    "                    else: # 받침있는 단어(종성) + 받침없는 단어(초성,중성)\n",
    "                        # 받침있는 단어의 종성이 없는 경우(ex: 아,녕) (앞글자 상관없음)\n",
    "                        candidate = word[:i+2]+candidates_letter[j]+word[i+2:] \n",
    "                        candidates.append(candidate)\n",
    "                        \n",
    "                        # 앞글자가 받침이 있을 때 현재 단어의 초성이 없는 경우(ex: 핸,드,폰->해,느,폰)\n",
    "                        candidate = word[:i-1]+word[i]+word[i-1]+candidates_letter[j]+word[i+1:] \n",
    "                        candidates.append(candidate)\n",
    "                        # 앞글자가 받침이 없을 때 현재 단어의 초성이 없는 경우(ex: 모,기,약->모,ㅣ,약)\n",
    "                        candidate = word[:i]+candidates_letter[j]+word[i:] \n",
    "                        candidates.append(candidate)\n",
    "                        # 위와 동일하지만 특수한 경우(ex: 고,기,압->괴압)\n",
    "                        candidate = word[:i-2]+word[i-1]+candidates_letter[j]+word[i-2]+word[i-1:] \n",
    "                        candidates.append(candidate)\n",
    "                        \n",
    "                        # 앞글자가 받침이 없을 때 현재 단어의 중성이 없는 경우(ex: 모,기,약->목,약)\n",
    "                        candidate = word[:i-2]+word[i-1]+word[i-2]+candidates_letter[j]+word[i-1:]  \n",
    "                        candidates.append(candidate)\n",
    "                        # 앞글자가 받침이 있을 때 현재 단어의 중성이 없는 경우(ex: 핸,드,폰->핸,ㄷ,폰)\n",
    "                        candidate = word[:i+1]+candidates_letter[j]+word[i+1:]  \n",
    "                        candidates.append(candidate)\n",
    "                    \n",
    "        #print(candidates)\n",
    "        return candidates\n",
    "\n",
    "    # Method : getCandidates(self, word)\n",
    "    # A method to generate set of candidates for a givne word using Damerau-Levenshtein edit distance\n",
    "    can_dict = list()\n",
    "    def genCandidates(self, word): # 자소 분리한 입력 단어를 매개변수로 받음\n",
    "        # Construct dictionary data structure\n",
    "        global can_dict\n",
    "        can_dict = self.extractCan(word)\n",
    "      #  print('can_dict로 후보 추출한 결과 : ', can_dict)\n",
    "        candidates = dict()\n",
    "        \n",
    "        for item in can_dict:\n",
    "            compose_item = self.uniteWord(item)\n",
    "            compose_item = hgtk.text.compose(compose_item)\n",
    "            if compose_item in self.dict:\n",
    "                # Along for loop, it calculate Damerau-Levenshtein edit distance between word and item with respect to all words corpus\n",
    "                distance = self.editDistanceOfDamerauLevenshtein(word, item)\n",
    "                # If Damerau-Levenshtein edit distance is less than 1,\n",
    "                # it assigns distance calculated above to candidates dictionary as key-value(item, distance).\n",
    "                #if item[-1]=='*'  and distance <= 2:\n",
    "                #    print(item[-1])\n",
    "                #    candidates[item] = distance\n",
    "         #       print('후보 : ', item, '거리 : ', distance)\n",
    "                if distance <= 1:\n",
    "                    candidates[item] = distance\n",
    "       #             print('candidate:',item)\n",
    "                \n",
    "\n",
    "        # Return sorted dictionary data structure\n",
    "        return sorted(candidates, key = candidates.get, reverse = False)\n",
    "\n",
    "    # Method : editType(self, candidate, word)\n",
    "    # A method to calculate edit type for single edit errors\n",
    "    # It can be one of four type, addition, deletion, substitution, reverse\n",
    "    def editType(self, candidate, word):\n",
    "        # The variable for classfying type of confusion matrix. Now it has default value, false.\n",
    "        edit = [False] * 4\n",
    "        correct = \"\"\n",
    "        error = \"\"\n",
    "        x = ''\n",
    "        w = ''\n",
    "        word = re.sub('ᴥ','',word)\n",
    "        candidate = re.sub('ᴥ','',candidate) #원활한 error, correct 계산을 위해 자소 분리에서 생성된 'ᴥ' 임시 제거\n",
    "        # Classifying the type of edit by Damerau-Levenshtein edit distance\n",
    "        # edit[0] = Insertion\n",
    "        # edit[1] = Deletion\n",
    "        # edit[2] = Substitution\n",
    "        # edit[3] = Transposition\n",
    "        minimum = min([len(word),len(candidate)]) \n",
    "        if minimum == 1 :\n",
    "        #for i in range(minimum):\n",
    "        # del :\n",
    "             i = 0\n",
    "             if len(candidate) > len(word):\n",
    "                if candidate[i:] == word[i-1:]: #Deletion\n",
    "                    edit[1] = True\n",
    "                    correct = candidate[i-1]\n",
    "                    error = ''\n",
    "                    x = candidate[i-2]\n",
    "                    w = candidate[i-2] + candidate[i-1]\n",
    "                  #  break\n",
    "                \n",
    "                elif len(candidate) < len(word): #Insertion \n",
    "                    correct = ''\n",
    "                    error = word[i]\n",
    "                    if i == 0:\n",
    "                        w = '#'\n",
    "                        x = '#' + error\n",
    "                    else:\n",
    "                        w = word[i-1]\n",
    "                        x = word[i-1] + error\n",
    "                        edit[0] = True\n",
    "                    #    break\n",
    "                \n",
    "                else :\n",
    "                    if candidate[i+1:] == word[i+1:]: #Substitution\n",
    "                        edit[2] = True\n",
    "                        correct = candidate[i]\n",
    "                        error = word[i]\n",
    "                        x = error\n",
    "                        w = correct\n",
    "                 #       break\n",
    "                \n",
    "                    elif candidate[i] == word[i+1] and candidate[i+2:] == word[i+2:]: #Transposition\n",
    "                        edit[3] = True\n",
    "                        correct = candidate[i] + candidate[i+1]\n",
    "                        error = word[i] + word[i+1]\n",
    "                        x = error\n",
    "                        w = correct\n",
    "                    #    break\n",
    "        else :\n",
    "            for i in range(min([len(word),len(candidate)])-1):\n",
    "                #print('word: ', word, 'candidate: ', candidate)\n",
    "                if candidate[0:i+1] != word[0:i+1]:\n",
    "                    if candidate[i:] == word[i-1:]: #Deletion\n",
    "                        edit[1] = True\n",
    "                        correct = candidate[i-1]\n",
    "                        error = ''\n",
    "                        x = candidate[i-2]\n",
    "                        w = candidate[i-2] + candidate[i-1]\n",
    "                        break\n",
    "\n",
    "                    elif candidate[i:] == word[i+1:]: #Insertion \n",
    "                        correct = ''\n",
    "                        error = word[i]\n",
    "                        if i == 0:\n",
    "                            w = '#'\n",
    "                            x = '#' + error\n",
    "                        else:\n",
    "                            w = word[i-1]\n",
    "                            x = word[i-1] + error\n",
    "                            edit[0] = True\n",
    "                            break\n",
    "\n",
    "                    if candidate[i+1:] == word[i+1:]: #Substitution\n",
    "                        edit[2] = True\n",
    "                        correct = candidate[i]\n",
    "                        error = word[i]\n",
    "                        x = error\n",
    "                        w = correct\n",
    "                        break\n",
    "\n",
    "                    if candidate[i] == word[i+1] and candidate[i+2:] == word[i+2:]: #Transposition\n",
    "                        edit[3] = True\n",
    "                        correct = candidate[i] + candidate[i+1]\n",
    "                        error = word[i] + word[i+1]\n",
    "                        x = error\n",
    "                        w = correct\n",
    "                        break\n",
    "\n",
    "        # Reverses both matrix\n",
    "        candidate = candidate[::-1]\n",
    "        word = word[::-1]\n",
    "\n",
    "        minimum = min([len(word),len(candidate)]) \n",
    "        if minimum == 1 :\n",
    "        #for i in range(minimum):\n",
    "        # del :\n",
    "            i = 1\n",
    "            if len(candidate) > len(word):\n",
    "                if candidate[i:] == word[i-1:]: #Deletion\n",
    "                    edit[1] = True\n",
    "                    correct = candidate[i-1]\n",
    "                    error = ''\n",
    "                    x = candidate[i-2]\n",
    "                    w = candidate[i-2] + candidate[i-1]\n",
    "                #    break\n",
    "\n",
    "                elif len(candidate) < len(word): #Insertion \n",
    "                    correct = ''\n",
    "                    error = word[i]\n",
    "                    if i == 0:\n",
    "                        w = '#'\n",
    "                        x = '#' + error\n",
    "                    else:\n",
    "                        w = word[i-1]\n",
    "                        x = word[i-1] + error\n",
    "                        edit[0] = True\n",
    "                  #      break\n",
    "\n",
    "                else :\n",
    "                    if candidate[i+1:] == word[i+1:]: #Substitution\n",
    "                        edit[2] = True\n",
    "                        correct = candidate[i]\n",
    "                        error = word[i]\n",
    "                        x = error\n",
    "                        w = correct\n",
    "                     #   break\n",
    "\n",
    "                    elif candidate[i] == word[i+1] and candidate[i+2:] == word[i+2:]: #Transposition\n",
    "                        edit[3] = True\n",
    "                        correct = candidate[i] + candidate[i+1]\n",
    "                        error = word[i] + word[i+1]\n",
    "                        x = error\n",
    "                        w = correct\n",
    "                    #    break\n",
    "        # Classifying the type of edit by Damerau-Levenshtein edit distance once again\n",
    "        else :\n",
    "            for i in range(min([len(word),len(candidate)])-1):\n",
    "                if candidate[0:i+1] != word[0:i+1]:\n",
    "\n",
    "                    if candidate[i:] == word[i-1:]:\n",
    "                        edit[1] = True\n",
    "                        correct = candidate[i-1]\n",
    "                        error = ''\n",
    "                        x = candidate[i-2]\n",
    "                        w = candidate[i-2] + candidate[i-1]\n",
    "                        break\n",
    "\n",
    "                    elif candidate[i:] == word[i+1:]:    \n",
    "                        correct = ''\n",
    "                        error = word[i]\n",
    "                        if i == 0:\n",
    "                            w = '#'\n",
    "                            x = '#'+error\n",
    "\n",
    "                        else:\n",
    "                            w = word[i-1]\n",
    "                            x = word[i-1] + error\n",
    "                            edit[0] = True\n",
    "                            break\n",
    "\n",
    "                    if candidate[i+1:] == word[i+1:]:\n",
    "                        edit[2] = True\n",
    "                        correct = candidate[i]\n",
    "                        error = word[i]\n",
    "                        x = error\n",
    "                        w = correct\n",
    "                        break\n",
    "\n",
    "                    if candidate[i] == word[i+1] and candidate[i+2:] == word[i+2:]:\n",
    "                        edit[3] = True\n",
    "                        correct = candidate[i] + candidate[i+1]\n",
    "                        error = word[i] + word[i+1]\n",
    "                        x = error\n",
    "                        w = correct\n",
    "                        break\n",
    "        # Returns the variables including edit types, correct ,error, x, w\n",
    "        # Example :\n",
    "        # Error | Correction | Correct Letter | Error Letter | Position(Letter #) | Type\n",
    "        # acress   actress           t              ----            2               deletion\n",
    "        # acress    cress          ----              a              0               insertion\n",
    "        # acress    caress          ca              ac              0             transposition\n",
    "        # acress    access          c               r               2               substitution\n",
    "        # acress    across          o               e               3               substitution\n",
    "        # acress    acres          ----             s               5               insertion\n",
    "        # acress    acres          ----             s               4               insertion\n",
    "   #     print(\"word : \", word, \"candidate : \", candidate)\n",
    "        if word == candidate:\n",
    "            return \"None\", '', '', '', ''\n",
    "        \n",
    "        if edit[1]:\n",
    "            return \"Deletion\", correct, error, x, w\n",
    "        \n",
    "        elif edit[0]:\n",
    "            return \"Insertion\", correct, error, x, w\n",
    "        \n",
    "        elif edit[2]:\n",
    "            return \"Substitution\", correct, error, x, w\n",
    "        \n",
    "        elif edit[3]:\n",
    "            return \"Reversal\", correct, error, x, w\n",
    "\n",
    "    # Method : channelMode(self, x, y, edit)\n",
    "    # A method to calculate channel model probability for errors\n",
    "    def channelModel(self, x,y, edit):\n",
    "        corpus = ' '.join(self.words)   #ng_word\n",
    "        \n",
    "        #print(\"x:\",x,\"y:\",y,\"edit:\",edit)\n",
    "        \n",
    "        if edit == 'add':\n",
    "            if x == '#':\n",
    "       #         print(\"return:\",self.addmatrix[(self.addmatrix['x']==x)&(self.addmatrix['y']==y)]['count']/(corpus.count(' '+y)+0.1))\n",
    "                return (self.addmatrix[(self.addmatrix['x']==x)&(self.addmatrix['y']==y)]['count']/(corpus.count(' '+y)+0.1)).values[0]\n",
    "            else:\n",
    "         #       print(\"return:\",self.addmatrix[(self.addmatrix['x']==x)&(self.addmatrix['y']==y)]['count']/(corpus.count(x)+0.1))\n",
    "                return (self.addmatrix[(self.addmatrix['x']==x)&(self.addmatrix['y']==y)]['count']/(corpus.count(x)+0.1)).values[0]\n",
    "            \n",
    "        if edit == 'sub':\n",
    "        #    print(\"return:\",self.submatrix[(self.submatrix['x']==x)&(self.submatrix['y']==y)]['count']/(corpus.count(y)+0.1))\n",
    "            return (self.submatrix[(self.submatrix['x']==x)&(self.submatrix['y']==y)]['count']/(corpus.count(y)+0.1)).values[0]\n",
    "        \n",
    "        if edit == 'rev':\n",
    "       #     print(\"return:\",self.revmatrix[(self.revmatrix['x']==x)&(self.revmatrix['y']==y)]['count']/(corpus.count(x+y)+0.1))\n",
    "            return (self.revmatrix[(self.revmatrix['x']==x)&(self.revmatrix['y']==y)]['count']/(corpus.count(x+y)+0.1)).values[0]\n",
    "        \n",
    "        if edit == 'del':\n",
    "      #      print(\"return:\",self.delmatrix[(self.delmatrix['x']==x)&(self.delmatrix['y']==y)]['count']/(corpus.count(x+y)+0.1))\n",
    "            return (self.delmatrix[(self.delmatrix['x']==x)&(self.delmatrix['y']==y)]['count']/(corpus.count(x+y)+0.1)).values[0]\n",
    "        \n",
    "######################################################################\n",
    "# section below starts main function to correct non-word spell errors.\n",
    "#\n",
    "# Overall Program Flow :\n",
    "# 1. Input the distorted sentence from user\n",
    "# 2. Generates the candidates which is answer corresponding to input\n",
    "# 3. Calculates edit distance by Damerau-Levenshtein edit distance algorithm\n",
    "# 4. Clasifying the edit type of candidates\n",
    "# 5. Calculates the sentence probability by selected #gram for each candidate\n",
    "# 6. Extracts the highest probability as correct answer\n",
    "# 7. Ends program\n",
    "######################################################################\n",
    "\n",
    "# Display the functions of NoisyChannelModel class\n",
    "# help(NoisyChannelModel)\n",
    "# Construct a instance of NoisyChannelModel class\n",
    "josa_word = list()    # 조사 확정 후 저장할 곳 \n",
    "\n",
    "def detach(word):\n",
    "\n",
    "    global josa_word\n",
    "    word_pos_list = list()    # 후보들\n",
    "    word_select = \"\" #원래 단어에서 조사 삭제하고 다시 리턴할 단어\n",
    "    word_pos_list = komoran.pos(word)\n",
    "    leng = len(josa_word)\n",
    "   # print(\"word_pos_list : \",word_pos_list)\n",
    "    for i in range(len(word_pos_list)):\n",
    "  #      print(\"twit : \",twitter.pos(word))\n",
    "        if 'J' in word_pos_list[i][1] :\n",
    "            josa_word.append(word_pos_list[i][0]) # 조사 저장\n",
    "            word_select=re.sub(word_pos_list[i][0],'',word) # 조사 삭제한 단어 저장\n",
    "        elif twitter.pos(word_pos_list[i][1]) == 'KoreanParticle' :\n",
    "            #print(\"particle\") #글자내에 ㅏ,ㅓ 같은 자모가 있으면 조사라고 인식하지 못함. ->맨끝 1단어 2단어 검색후 조사면 떼어냄.\n",
    "            if i != len(word)-1:\n",
    "                if 'J' in twitter.pos(word[-1])[1] : #맨끝 한글자가 조사\n",
    "                    josa_word.append(word[-1])\n",
    "                    word_select = word[:-1]\n",
    "                if 'J' in twitter.pos(word[-2:])[1]  : \n",
    "                    if 'J' in twitter.pos(word[-1])[1] : #맨끝한글자가 조사인데 두글자도 조사\n",
    "                        josa_word[-1] = word[-2:]   #있던거 없애고 추가하기\n",
    "                        word_select = word[:-2]\n",
    "                    else :#맨끝한글자가 조사가 아닌데 두글자는 조사\n",
    "                        josa_word.append(word[-2:])\n",
    "                        word_select = word[:-2]\n",
    "            else :  #조사 오타인 경우\n",
    "                    word_select = word[:-1]\n",
    "                    josa_word.append(word[-1])\n",
    "        else :   # particle이 없음.\n",
    "            word_select = word\n",
    "    if len(josa_word) == leng: #단어에 새로운 조사가 없으면\n",
    "        josa_word.append('')\n",
    "   # print(\"word_select : \",word_select)\n",
    "    if word_select == '' :\n",
    "        word_select = josa_word[-1]\n",
    "        josa_word[-1] = ''\n",
    "    return  word_select\n",
    "\n",
    "\n",
    "noisyChannelModel = NoisyChannelModel()\n",
    "Sample = preprocessing.preprocess('test1').prepro()\n",
    "#print(Sample)\n",
    "count = 0\n",
    "while (count != len(Sample)):\n",
    "    # 입력한 문장의 띄어쓰기 모두 삭제\n",
    "    start = Sample[count] # str(input('Input : '))    \n",
    "    count = count+1\n",
    "    sentence = \"\"\n",
    "    for item in start :\n",
    "        if item == \" \":\n",
    "            continue\n",
    "        else :\n",
    "            sentence = sentence + item\n",
    "            \n",
    "    sentence = spacing(sentence)   # 올바른 띄어쓰기된 문장\n",
    "    sentence = sentence.split()   # 단어별로 분리 후 sentence 저장\n",
    "    print(sentence)\n",
    "    # 조사 제거\n",
    "    \n",
    "    words = list()\n",
    "    for word in sentence :   #for word in sentence[:-1]\n",
    "        detach_word = detach(word)\n",
    "        words.append(detach_word)\n",
    "        \n",
    "  #  words.append(sentence[-1])   # 마지막 단어(동사)하나 추가\n",
    "    sentence = words\n",
    "    #print(\"조사 제거 후 : \", sentence , josa_word)\n",
    "    # User can input a sentence which has been \"distorted\"\n",
    "    # sentence = str(input('Input: ')).split()        # 각 단어별로 쪼개서 sentence에 저장\n",
    "    # sentence = splitSentence(sentence)\n",
    "    \n",
    "    correct = \"\"\n",
    "\n",
    "    # Starts Noise channel model algorithm\n",
    "    for index, word in enumerate(sentence):    #enumerate : (index,단어)로 변환\n",
    "        print(\"현재 검색 단어 :\", word)\n",
    "        word_input = word\n",
    "        word = hgtk.text.decompose(word)       #word : 단어를 자소로 분리\n",
    "        word = noisyChannelModel.splitWord(word)  # splitword : 자소분리된 것 중 ㄲ,ㅒ등 분리\n",
    "        candidates = noisyChannelModel.genCandidates(word)   # 후보생성\n",
    "        \n",
    "        # Constructs two dictionary\n",
    "        NP = dict()\n",
    "        P = dict()\n",
    "\n",
    "        for item in candidates:\n",
    "            # Gets edit type of each candidate\n",
    "            edit = noisyChannelModel.editType(item, word)\n",
    "            \n",
    "            #print(\"edit:\",edit)\n",
    "            item = noisyChannelModel.uniteWord(item)\n",
    "            item = hgtk.text.compose(item)        # 밑에 넣어주려고 임시 결합\n",
    "            \n",
    "            # This section classify the edit type by Damerau-Levenshtein edit distance    # 거리 계산\n",
    "            if edit == None:  \n",
    "                continue\n",
    "            \n",
    "            if edit[0] == \"Insertion\":\n",
    "                NP[item] = noisyChannelModel.channelModel(edit[3][0],edit[3][1], 'add')\n",
    "                \n",
    "            if edit[0] == 'Deletion':\n",
    "                NP[item] = noisyChannelModel.channelModel(edit[4][0], edit[4][1], 'del')\n",
    "                \n",
    "            if edit[0] == 'Reversal':\n",
    "                NP[item] = noisyChannelModel.channelModel(edit[4][0], edit[4][1], 'rev')\n",
    "                \n",
    "            if edit[0] == 'Substitution':\n",
    "                NP[item] = noisyChannelModel.channelModel(edit[3], edit[4], 'sub')\n",
    "\n",
    "        #print(\"Noise : \", NP)\n",
    "        # Calculates probability with respect to items in NP dictionary      확률계산\n",
    "        for item in NP:\n",
    "            # Assigns the probability of NP value by item key\n",
    "            channel = NP[item]\n",
    "\n",
    "            # Calculates trigram sentence probability / 이름은 수정할게 많아져서\n",
    "            if len(sentence)-1 != index:\n",
    "                bigram = calc.pow(calc.e, noisyChannelModel.ng.sentenceprobability(sentence[index-1]+item+sentence[index+1], 'bi', 'antilog'))\n",
    "                \n",
    "            else:\n",
    "                bigram = calc.pow(calc.e, noisyChannelModel.ng.sentenceprobability(sentence[index-1]+item, 'bi', 'antilog'))\n",
    "\n",
    "            # Assigns final probability into P dictionary\n",
    "            P[item] =  channel * bigram * calc.pow(10,9)   #channel *\n",
    "\n",
    "        # Sorts as parameters    확률 제일 높은거 저장\n",
    "        P = sorted(P,key = P.get, reverse = True) #   lambda x: x[1]\n",
    "\n",
    "        #print('bi*noise : ', P)\n",
    "        if P == []:\n",
    "            #correct = correct + word + ' '\n",
    "            w =  noisyChannelModel.uniteWord(word)\n",
    "            w = hgtk.text.compose(w)\n",
    "            Res = 1\n",
    "           #print(\"P가 없어 동사활용인지 검색 : \",w)\n",
    "            if twitter.pos(w)[0][1] == 'Verb':\n",
    "                for i in range(len(twitter.pos(w))):\n",
    "                    if twitter.pos(w)[i][1]==\"KoreanParticle\":\n",
    "                        print('word has particle')\n",
    "                        correct = correct +w+ ' ' \n",
    "                        Res = 0\n",
    "                        break\n",
    "                if Res ==1 :\n",
    "                    correct = correct +word+ ' ' \n",
    "                    #print('활용동사')\n",
    "            else :\n",
    "                P_can = list()\n",
    "                for w in can_dict :\n",
    "                    w =  noisyChannelModel.uniteWord(w)\n",
    "                    w = hgtk.text.compose(w)\n",
    "                    if len(hannanum.pos(w)) != 0 and hannanum.pos(w)[0][1] == 'P' :\n",
    "                        for i in range(len(twitter.pos(w))):\n",
    "                            if twitter.pos(w)[i][1]==\"KoreanParticle\":\n",
    "                                Res = 0\n",
    "                                break\n",
    "                        if Res == 1 :\n",
    "                            P_can.append(w)\n",
    "                P_can = list(set(P_can))\n",
    "                if P_can == [] :\n",
    "                    correct = correct + word + ' '\n",
    "                else :\n",
    "                    correct = correct + P_can[0] + ' '\n",
    "                #print(\"P_can :\", P_can)\n",
    "\n",
    "            \n",
    "         #   P.append('')\n",
    "        # Extracts the candidate which has highest probability\n",
    "        else :\n",
    "            correct = correct +P[0] + ' '       #correct : 단어 + 띄어쓰기\n",
    "            \n",
    "    # Prints the output response respect to input sentence iteratively\n",
    "    correct = noisyChannelModel.uniteWord(correct)   # ㄲ,ㅒ 합치기\n",
    "    correct = hgtk.text.compose(correct)             # 단어 합치기\n",
    "   # print(\"correct : \", correct)\n",
    "    correct = correct.split()\n",
    "    #print(\"correct : \" ,correct,\"합 : \",correct + josa_word)      \n",
    "    minimum = min(len(correct),len(josa_word))\n",
    "    for i in range(0,minimum) :\n",
    "        correct[i] = correct[i] + josa_word[i]\n",
    "    print('Response: ',\" \".join(correct))\n",
    "    \n",
    "    gathering_sen = []\n",
    "    gathering_sen.append(\" \".join(correct))\n",
    "    \n",
    "    josa_word.clear()\n",
    "# End program\n",
    "for sen in gathering_sen:\n",
    "    print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
